{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision wandb transformers huggingface_hub datasets gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EMrcce1KRPHl",
        "outputId": "627745de-c366-4258-b5ee-310febd29a2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.43.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, zipfile, urllib.request\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ],
      "metadata": {
        "id": "GS3KWDesR1De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02502701-d5a0-46a4-8a73-c13c546e4364"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not os.path.exists(\"tiny-imagenet-200\"):\n",
        "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    urllib.request.urlretrieve(url, \"tiny-imagenet-200.zip\")\n",
        "    with zipfile.ZipFile(\"tiny-imagenet-200.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"Downloaded and extracted Tiny ImageNet.\")\n",
        "else:\n",
        "    print(\"Dataset already available.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eLJlDp6T-5B",
        "outputId": "de2a1d3f-580e-45dc-dda5-fdf7a4a77a41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "\n",
        "data_dir = \"tiny-imagenet-200\"\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "val_dir = os.path.join(data_dir, \"val\")\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "train_data_full = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "\n",
        "\n",
        "val_annotations = os.path.join(val_dir, \"val_annotations.txt\")\n",
        "df_val = pd.read_csv(val_annotations, sep=\"\\t\", header=None,\n",
        "                     names=[\"file\", \"class\", \"x1\", \"y1\", \"x2\", \"y2\"])\n",
        "val_class_map = dict(zip(df_val[\"file\"], df_val[\"class\"]))\n",
        "\n",
        "\n",
        "val_images = []\n",
        "for fname in os.listdir(os.path.join(val_dir, \"images\")):\n",
        "    if fname in val_class_map:\n",
        "        wnid = val_class_map[fname]\n",
        "        if wnid in train_data_full.class_to_idx:\n",
        "            label = train_data_full.class_to_idx[wnid]\n",
        "            img_path = os.path.join(val_dir, \"images\", fname)\n",
        "            val_images.append((img_path, label))\n",
        "\n",
        "print(f\" Validation images remapped: {len(val_images)} samples across {len(set(val_class_map.values()))} classes.\")\n",
        "\n",
        "\n",
        "class TinyImageNetValDataset(Dataset):\n",
        "    def __init__(self, samples, transform=None, loader=default_loader):\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "        self.loader = loader\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = self.loader(path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "val_data_full = TinyImageNetValDataset(val_images, transform=val_transforms)\n",
        "\n",
        "\n",
        "def balanced_subset(dataset, n_per_class):\n",
        "    \"\"\"\n",
        "    Returns a subset with up to n_per_class samples per class.\n",
        "    Works for both train and val datasets.\n",
        "    \"\"\"\n",
        "    if hasattr(dataset, \"samples\"):\n",
        "        labels = [lbl for _, lbl in dataset.samples]\n",
        "    elif hasattr(dataset, \"samples_list\"):\n",
        "        labels = [lbl for _, lbl in dataset.samples_list]\n",
        "    else:\n",
        "\n",
        "        labels = [lbl for _, lbl in dataset.samples]\n",
        "\n",
        "    unique_labels = sorted(set(labels))\n",
        "    indices = []\n",
        "    for c in unique_labels:\n",
        "        c_indices = [i for i, l in enumerate(labels) if l == c]\n",
        "        choose = min(n_per_class, len(c_indices))\n",
        "        indices.extend(random.sample(c_indices, choose))\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "\n",
        "n_per_class_train = 100  # 200 * 200 = 40,000 training samples\n",
        "n_per_class_val   = 7     # 7 * 200 = 1,400 validation samples\n",
        "\n",
        "train_data = balanced_subset(train_data_full, n_per_class_train)\n",
        "val_data   = balanced_subset(val_data_full, n_per_class_val)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_data, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Using balanced subset: {len(train_data)} train samples, {len(val_data)} val samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3-Q9IgHR-VA",
        "outputId": "96470008-7fc9-4c7d-c78e-1bffeb47fa30"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation images remapped: 10000 samples across 200 classes.\n",
            "Using balanced subset: 20000 train samples, 1400 val samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map wnid -> human name via words.txt\n",
        "words_path = os.path.join(data_dir, \"words.txt\")\n",
        "id_to_name = {}\n",
        "with open(words_path) as f:\n",
        "    for line in f:\n",
        "        wnid, name = line.strip().split(\"\\t\")\n",
        "        id_to_name[wnid] = name.split(\",\")[0]\n",
        "\n",
        "class_ids = train_data_full.classes\n",
        "# Count occurrences in balanced subset\n",
        "subset_labels = [train_data_full.samples[i][1] for i in train_data.indices]\n",
        "from collections import Counter\n",
        "cnt = Counter(subset_labels)\n",
        "top5 = cnt.most_common(5)\n",
        "print(\"Top 5 classes in subset (readable name, wnid, count):\")\n",
        "for idx, c in top5:\n",
        "    wnid = class_ids[idx]\n",
        "    print(f\"{id_to_name.get(wnid, wnid)} ({wnid}): {c}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U_o8EKIS3ow",
        "outputId": "79aea8c9-0b34-4590-a684-8113800909a4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 classes in subset (readable name, wnid, count):\n",
            "goldfish (n01443537): 100\n",
            "European fire salamander (n01629819): 100\n",
            "bullfrog (n01641577): 100\n",
            "tailed frog (n01644900): 100\n",
            "American alligator (n01698640): 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"TinyImageNet-ResNet\", name=\"resnet34_finetune_balanced_augmented\", reinit=True)\n",
        "wandb.config.update({\n",
        "    \"model\": \"resnet34-pretrained\",\n",
        "    \"dataset\": \"tiny-imagenet-200 (balanced subset, strong aug)\",\n",
        "    \"train_samples\": len(train_data),\n",
        "    \"val_samples\": len(val_data),\n",
        "    \"batch_size\": 64,\n",
        "    \"n_per_class_train\": n_per_class_train,\n",
        "    \"n_per_class_val\": n_per_class_val,\n",
        "    \"optimizer\": \"AdamW\",\n",
        "    \"lr_warmup\": 1e-3,\n",
        "    \"lr_finetune\": 5e-5,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"epochs\": 7\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "PRHHCIfKtl-r",
        "outputId": "8c944135-d059-4d8a-c3d4-b094de2a22ac"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>baseline_accuracy</td><td>▁</td></tr><tr><td>drifted_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>baseline_accuracy</td><td>0.69357</td></tr><tr><td>drifted_accuracy</td><td>0.2147</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet34_drift_eval</strong> at: <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/64u5vdj8' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/64u5vdj8</a><br> View project at: <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251109_163503-64u5vdj8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251109_170011-dm2jrbs9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/dm2jrbs9' target=\"_blank\">resnet34_finetune_balanced_augmented</a></strong> to <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/dm2jrbs9' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/dm2jrbs9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, mcp] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_text = \"\\n\".join([f\"{k}: {v}\" for k, v in wandb.config.items()])\n",
        "wandb.log({\"training_config\": wandb.Html(f\"<pre>{config_text}</pre>\")})\n"
      ],
      "metadata": {
        "id": "At-BAsALj1uh"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "in_feats = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(in_feats, 200)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n"
      ],
      "metadata": {
        "id": "HRSfaX7fS6cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302c6652-b4df-4a75-f68f-d8fd1b659ade"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 174MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"fc\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "warmup_epochs = 3\n"
      ],
      "metadata": {
        "id": "g9OWBhHOV3F7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Warmup training (classifier head only)...\")\n",
        "\n",
        "for epoch in range(1, warmup_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Warmup Epoch {epoch}/{warmup_epochs}\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_loss = running_loss / total\n",
        "    wandb.log({\"warmup_epoch\": epoch, \"train_acc\": train_acc, \"train_loss\": train_loss})\n",
        "    print(f\"Warmup Epoch {epoch}: Train Acc {train_acc:.3f}, Train Loss {train_loss:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBmQMgT6osFW",
        "outputId": "828df471-39b3-42cd-c2da-75ca87aa9d14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup training (classifier head only)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warmup Epoch 1/3: 100%|██████████| 625/625 [01:46<00:00,  5.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup Epoch 1: Train Acc 0.143, Train Loss 4.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warmup Epoch 2/3: 100%|██████████| 625/625 [01:38<00:00,  6.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup Epoch 2: Train Acc 0.231, Train Loss 3.958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warmup Epoch 3/3: 100%|██████████| 625/625 [01:54<00:00,  5.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup Epoch 3: Train Acc 0.250, Train Loss 3.902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=5e-5,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=12\n",
        ")\n",
        "\n",
        "print(\"Starting full fine-tuning of entire ResNet34...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLBYAAxMo09S",
        "outputId": "5a182ab7-7c26-4f86-e59e-1ef3446e06fe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting full fine-tuning of entire ResNet34...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = 0.0\n",
        "epochs = 7\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Fine-tune Epoch {epoch}/{epochs}\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        train_correct += preds.eq(labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_acc = train_correct / train_total\n",
        "    train_loss /= train_total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            val_correct += preds.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Acc {train_acc:.3f} | Val Acc {val_acc:.3f}\")\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + warmup_epochs,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_acc\": val_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"lr\": scheduler.get_last_lr()[0]\n",
        "    })\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"resnet34_tinyimagenet_best.pth\")\n",
        "        artifact = wandb.Artifact(\"resnet34_tinyimagenet\", type=\"model\")\n",
        "        artifact.add_file(\"resnet34_tinyimagenet_best.pth\")\n",
        "        wandb.log_artifact(artifact)\n"
      ],
      "metadata": {
        "id": "PTRLkhRYo4SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f748a0-d6b1-47c5-91dd-4da8346d5794"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tune Epoch 1/7: 100%|██████████| 313/313 [01:54<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc 0.272 | Val Acc 0.570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tune Epoch 2/7: 100%|██████████| 313/313 [01:53<00:00,  2.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Acc 0.353 | Val Acc 0.638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tune Epoch 3/7: 100%|██████████| 313/313 [01:55<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Acc 0.415 | Val Acc 0.651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tune Epoch 4/7: 100%|██████████| 313/313 [01:57<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Acc 0.455 | Val Acc 0.661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tune Epoch 5/7: 100%|██████████| 313/313 [01:52<00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Acc 0.498 | Val Acc 0.682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tune Epoch 6/7: 100%|██████████| 313/313 [01:53<00:00,  2.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Acc 0.527 | Val Acc 0.682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-tune Epoch 7/7: 100%|██████████| 313/313 [01:53<00:00,  2.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Acc 0.554 | Val Acc 0.694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training complete. Best Validation Accuracy:\", best_val_acc)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "aIF2rji4YlOJ",
        "outputId": "3843dd87-e015-4d5b-be83-880c622a14c0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete. Best Validation Accuracy: 0.6935714285714286\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>lr</td><td>▁▅▇█▇▆▅</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▄▃▂▂▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇█</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>lr</td><td>3e-05</td></tr><tr><td>train_acc</td><td>0.55385</td></tr><tr><td>train_loss</td><td>2.57283</td></tr><tr><td>val_acc</td><td>0.69357</td></tr><tr><td>val_loss</td><td>2.01699</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet34_finetune_balanced_augmented</strong> at: <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/plarv3mu' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/plarv3mu</a><br> View project at: <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet</a><br>Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251109_161729-plarv3mu/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet34(weights=None)\n",
        "in_feats = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(in_feats, 200)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"resnet34_tinyimagenet_best.pth\", map_location=device))\n",
        "model.eval()\n",
        "print(\"Loaded best trained model for evaluation.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h57oDH2jdc0u",
        "outputId": "62421c7e-2b4b-4499-895a-7c39549994a5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best trained model for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"TinyImageNet-ResNet\", name=\"resnet34_drift_eval\", reinit=True)\n",
        "wandb.config.update({\n",
        "    \"phase\": \"drift_evaluation\",\n",
        "    \"model\": \"resnet34_finetuned\",\n",
        "    \"dataset\": \"tiny-imagenet-200\",\n",
        "})\n",
        "print(\"W&B reinitialized for drift evaluation logging.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "lOfbX59ZeJPK",
        "outputId": "befa3081-71dd-486b-8cfb-283740984d3b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251109_163503-64u5vdj8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/64u5vdj8' target=\"_blank\">resnet34_drift_eval</a></strong> to <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/64u5vdj8' target=\"_blank\">https://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/TinyImageNet-ResNet/runs/64u5vdj8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B reinitialized for drift evaluation logging.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "drift_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ColorJitter(brightness=0.8, contrast=0.8),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x + 0.05 * torch.randn_like(x)),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_data_drifted = TinyImageNetValDataset(val_images, transform=drift_transforms)\n",
        "val_loader_drifted = DataLoader(val_data_drifted, batch_size=64, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "HbP_0X53b2A5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "baseline_acc = evaluate_model(model, val_loader)\n",
        "drifted_acc = evaluate_model(model, val_loader_drifted)\n",
        "\n",
        "print(f\"Baseline Accuracy: {baseline_acc*100:.2f}%\")\n",
        "print(f\"Drifted Accuracy:  {drifted_acc*100:.2f}%\")\n",
        "\n",
        "wandb.log({\"baseline_accuracy\": baseline_acc, \"drifted_accuracy\": drifted_acc})\n",
        "\n",
        "if drifted_acc < 0.8 * baseline_acc:\n",
        "    wandb.alert(\n",
        "        title=\"Model Accuracy Drift Detected\",\n",
        "        text=f\"Accuracy dropped from {baseline_acc*100:.2f}% to {drifted_acc*100:.2f}%\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6DY-5Rnb4r_",
        "outputId": "9c3399db-5d89-43ec-ddcd-c58680a8f8e8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 69.36%\n",
            "Drifted Accuracy:  21.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import sys\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_model():\n",
        "    model = models.resnet34(weights=None)\n",
        "    in_feats = model.fc.in_features\n",
        "    model.fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(in_feats, 200))\n",
        "    if os.path.exists(\"resnet34_tinyimagenet_best.pth\"):\n",
        "        model.load_state_dict(torch.load(\"resnet34_tinyimagenet_best.pth\", map_location=device))\n",
        "    else:\n",
        "        print(\" Model weights not found!\", file=sys.stderr)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "\n",
        "id_to_name = {}\n",
        "if os.path.exists(\"words.txt\"):\n",
        "    with open(\"words.txt\", \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) == 2:\n",
        "                wnid, name = parts\n",
        "                id_to_name[wnid] = name.split(\",\")[0].strip()\n",
        "\n",
        "\n",
        "if os.path.exists(\"wnids.txt\"):\n",
        "    with open(\"wnids.txt\", \"r\") as f:\n",
        "        class_ids = [line.strip() for line in f if line.strip()]\n",
        "else:\n",
        "    print(\"wnids.txt not found! Using alphabetical order of words.txt.\", file=sys.stderr)\n",
        "    class_ids = sorted(list(id_to_name.keys()))\n",
        "\n",
        "\n",
        "idx_to_label = [id_to_name.get(wnid, wnid) for wnid in class_ids]\n",
        "\n",
        "def predict(image):\n",
        "    image = val_transforms(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        probs = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "        top5 = torch.topk(probs, 5)\n",
        "\n",
        "    results = {}\n",
        "    for idx, prob in zip(top5.indices, top5.values):\n",
        "        wnid = class_ids[idx.item()] if idx.item() < len(class_ids) else f\"class_{idx.item()}\"\n",
        "        readable = id_to_name.get(wnid, wnid)\n",
        "        results[f\"{readable} ({wnid})\"] = round(float(prob.item()), 4)\n",
        "    return results\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
        "    outputs=gr.Label(num_top_classes=5, label=\"Top-5 Predictions\"),\n",
        "    title=\"Tiny ImageNet Classifier (ResNet-34)\",\n",
        "    description=\"Upload an image to see Top-5 predicted Tiny ImageNet classes with WNIDs.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_name=\"0.0.0.0\", server_port=7860)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS9etw9ddzWU",
        "outputId": "2fd689dd-9011-47db-a86b-cbd66aa8d0a1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "torch\n",
        "torchvision\n",
        "gradio\n",
        "Pillow\n",
        "numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flLPrvqEfZFm",
        "outputId": "36adaef2-9c06-4021-cdda-431ddfc5bdf7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlfhM5HrgDHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}