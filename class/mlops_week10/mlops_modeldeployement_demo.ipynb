{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAxUQ-G4folT",
    "outputId": "4a2f35c5-5296-47a0-fd85-1cf958dc1b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Environment Validation ----\n",
      "GPU available: False\n",
      "HF_TOKEN: NOT SET\n",
      "HF_USER: NOT SET\n",
      "SPACE_NAME: NOT SET\n",
      "WANDB_API_KEY: ⚠️ Not set (you'll log in interactively)\n",
      "Dependencies:  wandb, gradio, huggingface_hub imported successfully\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "#@title 🔍 Environment Validation (run before starting lab)\n",
    "import torch, os\n",
    "\n",
    "print(\"---- Environment Validation ----\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Check required env vars\n",
    "for var in [\"HF_TOKEN\", \"HF_USER\", \"SPACE_NAME\"]:\n",
    "    val = os.environ.get(var)\n",
    "    print(f\"{var}:\", \" SET\" if val else \"NOT SET\")\n",
    "\n",
    "# Check if W&B API key configured (login handled separately)\n",
    "wandb_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "print(\"WANDB_API_KEY:\", \" SET\" if wandb_key else \"⚠️ Not set (you'll log in interactively)\")\n",
    "\n",
    "# Check if dependencies installed\n",
    "try:\n",
    "    import wandb, gradio, huggingface_hub\n",
    "    print(\"Dependencies:  wandb, gradio, huggingface_hub imported successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Dependencies: missing - please run install cell first\")\n",
    "    print(e)\n",
    "\n",
    "print(\"---------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2Ks8ugIfuij",
    "outputId": "69dcea6a-eaa9-45e5-a74e-7e3f16016337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m159.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m161.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.0/381.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q wandb gradio huggingface_hub git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jo269tBf5M-",
    "outputId": "4f394e39-1f90-43ee-fe4f-0161bfd24f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your Hugging Face token when prompted. It will be hidden.\n",
      "Hugging Face token: ··········\n",
      "Enter your Hugging Face username (e.g. 'alice'): Umamahesh1226\n",
      "Enter desired Space name (e.g. 'cifar100-demo-space'): week_10\n",
      "HF_TOKEN stored in runtime (hidden). HF_USER and SPACE_NAME saved in environment variables.\n"
     ]
    }
   ],
   "source": [
    "#@title 3) Securely set your Hugging Face token, username, and desired Space name\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "print(\"Paste your Hugging Face token when prompted. It will be hidden.\")\n",
    "hf_token = getpass(\"Hugging Face token: \")\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "# Edit these values (do NOT put the token here)\n",
    "hf_user = input(\"Enter your Hugging Face username (e.g. 'alice'): \").strip()\n",
    "space_name = input(\"Enter desired Space name (e.g. 'cifar100-demo-space'): \").strip()\n",
    "\n",
    "os.environ['HF_USER'] = hf_user\n",
    "os.environ['SPACE_NAME'] = space_name\n",
    "\n",
    "print(\"HF_TOKEN stored in runtime (hidden). HF_USER and SPACE_NAME saved in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "gKaHnglkgE6w",
    "outputId": "4c143460-e053-49bb-9f35-c33b6e2c921a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow the prompt to authenticate W&B (this opens an input box).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muma_mahesh_iitpkd\u001b[0m (\u001b[33muma_mahesh_iitpkd-indian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title 4) Authenticate Weights & Biases (W&B)\n",
    "import wandb\n",
    "print(\"Follow the prompt to authenticate W&B (this opens an input box).\")\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zwq9NLSLhPxk"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > train.py <<'PY'\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--project\", type=str, default=\"cifar100-hf-demo\")\n",
    "    p.add_argument(\"--entity\", type=str, default=None)\n",
    "    p.add_argument(\"--epochs\", type=int, default=5)\n",
    "    p.add_argument(\"--batch-size\", type=int, default=128)\n",
    "    p.add_argument(\"--lr\", type=float, default=0.01)\n",
    "    return p.parse_args()\n",
    "\n",
    "def get_dataloaders(batch_size):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4865, 0.4409),\n",
    "                             (0.2673, 0.2564, 0.2762)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4865, 0.4409),\n",
    "                             (0.2673, 0.2564, 0.2762)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "    testset  = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader  = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def train_one_epoch(model, device, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "def evaluate(model, device, loader, criterion):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            l = criterion(outputs, targets)\n",
    "            loss += l.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return loss/total, 100.*correct/total\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    wandb.init(project=args.project, entity=args.entity, config=vars(args))\n",
    "    cfg = wandb.config\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    trainloader, testloader = get_dataloaders(cfg.batch_size)\n",
    "\n",
    "    model = resnet18(num_classes=100)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, device, trainloader, optimizer, criterion)\n",
    "        test_loss, test_acc = evaluate(model, device, testloader, criterion)\n",
    "        wandb.log({\"epoch\": epoch+1, \"train_loss\": train_loss, \"train_acc\": train_acc,\n",
    "                   \"test_loss\": test_loss, \"test_acc\": test_acc})\n",
    "        print(f\"Epoch {epoch+1}: train_acc={train_acc:.2f} test_acc={test_acc:.2f}\")\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            os.makedirs(\"outputs\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), \"outputs/model.pt\")\n",
    "            # log artifact\n",
    "            artifact = wandb.Artifact(\"resnet18-cifar100\", type=\"model\", metadata={\"test_acc\": best_acc})\n",
    "            artifact.add_file(\"outputs/model.pt\")\n",
    "            wandb.log_artifact(artifact)\n",
    "    print(\"Best test acc:\", best_acc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqpWWWqnhoVU",
    "outputId": "00183c7b-77dc-462a-a90b-4134e732ce34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muma_mahesh_iitpkd\u001b[0m (\u001b[33muma_mahesh_iitpkd-indian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run gv9yj6au (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m setting up run gv9yj6au (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20251022_153343-gv9yj6au\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-paper-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/cifar100-umamahesh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/cifar100-umamahesh/runs/gv9yj6au\u001b[0m\n",
      "100% 169M/169M [00:04<00:00, 39.9MB/s]\n",
      "Epoch 1: train_acc=10.30 test_acc=17.66\n",
      "Epoch 2: train_acc=19.34 test_acc=23.66\n",
      "Epoch 3: train_acc=24.45 test_acc=28.18\n",
      "Best test acc: 28.18\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mpleasant-paper-1\u001b[0m at: \u001b[34mhttps://wandb.ai/uma_mahesh_iitpkd-indian-institute-of-technology/cifar100-umamahesh/runs/gv9yj6au\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251022_153343-gv9yj6au/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --project cifar100-umamahesh --entity uma_mahesh_iitpkd-indian-institute-of-technology --epochs 3 --batch-size 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pAPri4oiNXs",
    "outputId": "d7a3ed3e-191f-4ac5-e3d7-2380e0b67ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded artifact to outputs/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python - <<'PY'\n",
    "import wandb, os, sys\n",
    "ENTITY = os.environ.get(\"uma_mahesh_iitpkd-indian-institute-of-technology\") or \"uma_mahesh_iitpkd-indian-institute-of-technology\"   # <-- edit if not set\n",
    "PROJECT = \"cifar100-umamahesh\"\n",
    "ARTIFACT = \"resnet18-cifar100:latest\"\n",
    "api = wandb.Api()\n",
    "try:\n",
    "    artifact = api.artifact(f\"{ENTITY}/{PROJECT}/{ARTIFACT}\")\n",
    "    artifact.download(root=\"outputs\")\n",
    "    print(\"Downloaded artifact to outputs/\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to download artifact:\", e)\n",
    "    sys.exit(1)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Uo9BnRwLi-Y7"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > app.py <<'PY'\n",
    "import os, time, io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import gradio as gr\n",
    "\n",
    "MODEL_PATH = \"outputs/model.pt\"\n",
    "\n",
    "# If model not present, try download via W&B (requires WANDB_API_KEY secret in Space or env)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "        if wandb_api_key:\n",
    "            wandb.login(key=wandb_api_key)\n",
    "            api = wandb.Api()\n",
    "            artifact = api.artifact(os.environ.get(\"WANDB_ARTIFACT\", \"uma_mahesh_iitpkd-indian-institute-of-technology/cifar100-umamahesh/resnet18-cifar100:latest\"))\n",
    "            artifact.download(root=\"outputs\")\n",
    "            print(\"Downloaded model via W&B artifact.\")\n",
    "        else:\n",
    "            print(\"WANDB_API_KEY not set; cannot download artifact.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error downloading artifact via W&B:\", e)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = resnet18(num_classes=100)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409),(0.2673,0.2564,0.2762))\n",
    "])\n",
    "\n",
    "def predict_image(img):\n",
    "    start = time.time()\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        probs = torch.nn.functional.softmax(out, dim=1)\n",
    "        conf, idx = probs.max(1)\n",
    "        class_idx = int(idx.item())\n",
    "        conf_val = float(conf.item())\n",
    "    latency = (time.time() - start) * 1000.0\n",
    "    return {\"class_idx\": class_idx, \"confidence\": round(conf_val,4), \"latency_ms\": round(latency,2)}\n",
    "\n",
    "iface = gr.Interface(fn=predict_image, inputs=gr.Image(type=\"pil\"), outputs=\"json\", title=\"CIFAR-100 demo\")\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(server_name=\"0.0.0.0\", server_port=7860)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5IahAceJoSil"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > requirements.txt <<'REQ'\n",
    "torch\n",
    "torchvision\n",
    "gradio\n",
    "Pillow\n",
    "wandb\n",
    "huggingface_hub\n",
    "git-lfs\n",
    "REQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WKt940ZupY7n"
   },
   "outputs": [],
   "source": [
    "os.environ['SPACE_NAME']=\"mlops_week10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-uW0ixsoYyW",
    "outputId": "da8a4e87-f226-4b49-c677-7e052a3a57eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /content/hf_space/.git/\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "Repo URL: https://huggingface.co/spaces/Umamahesh1226/mlops_week10\n",
      "Pushed to: https://huggingface.co/spaces/Umamahesh1226/mlops_week10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Using 'master' as the name for the initial branch. This default branch name\n",
      "hint: is subject to change. To configure the initial branch name to use in all\n",
      "hint: of your new repositories, which will suppress this warning, call:\n",
      "hint: \n",
      "hint: \tgit config --global init.defaultBranch <name>\n",
      "hint: \n",
      "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
      "hint: 'development'. The just-created branch can be renamed via this command:\n",
      "hint: \n",
      "hint: \tgit branch -m <name>\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "# prepare local repo\n",
    "rm -rf hf_space || true\n",
    "mkdir hf_space\n",
    "cp app.py requirements.txt hf_space/\n",
    "cd hf_space\n",
    "\n",
    "git init\n",
    "git config user.email \"142502018@smail.iitpkd.ac.in\"\n",
    "git config user.name \"UmaMaheswarReddy-IIT-Pkd\"\n",
    "git lfs install\n",
    "\n",
    "python - <<'PY'\n",
    "from huggingface_hub import HfApi, Repository\n",
    "import os, sys\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "user = os.environ.get(\"HF_USER\")\n",
    "space = os.environ.get(\"SPACE_NAME\")\n",
    "if not token or not user or not space:\n",
    "    print(\"HF_TOKEN, HF_USER or SPACE_NAME not set. Aborting.\")\n",
    "    sys.exit(1)\n",
    "api = HfApi(token=token)\n",
    "\n",
    "repo_id = f\"{user}/{space}\"\n",
    "repo_url = api.create_repo(repo_id=repo_id, repo_type=\"space\",\n",
    "            space_sdk=\"gradio\",\n",
    "            exist_ok=True)\n",
    "print(\"Repo URL:\", repo_url)\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\".\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"space\",\n",
    "    commit_message=\"Initial commit: CIFAR-100 Gradio app (no model)\"\n",
    ")\n",
    "\n",
    "print(\"Pushed to:\", repo_url)\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsmqVOW6x30J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
